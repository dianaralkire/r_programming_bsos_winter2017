{
    "collab_server" : "",
    "contents" : "# Complete all of the items below\n# Use comments where you're having trouble or questions\n\n# 1. Read your data set into R\n# read.table (and varieties)\n# read.spss (from \"foreign\" package)\n\nscott <- read.csv(\"scottdata/CognitionPaperFinalData.csv\")\n# equivalent:\nscott <- read.table(\"scottdata/CognitionPaperFinalData.csv\", \n                    header = TRUE, sep = \",\")\n# library(foreign)\n# read.dta()  # for Stata .dta files, only for Stata < 13\n# mydata <- as.data.frame(read.dta(\"myfile.dta\"))\n# library(readstata13)\n# read.dta13()\n# library(memisc) # for other Stata\n# read.spss() # for .sav\n\n# tip: missing data\n# example: \"999\" means \"missing\"\n# read.csv(\"myfile.csv\", na.strings = c(999, \"n/a\", \"N/A\"))\n\n# 2. Peek at the top few rows\nhead(scott)\nscott[1:6, ] # equivalent\n\n# 3. Peek at the top few rows for only a few columns\nhead(scott[, 1:7])\nscott[1:6, 1:7] # equivalent\n\n# 4. How many rows does your data have?\nnrow(scott)\n\n# 5. Get a summary for every column\nsummary(scott)\n\n# 6. Get a summary for one column\nsummary(scott$WithinMinPairs)\nsummary(scott[ , \"WithinMinPairs\"])\nsummary(scott[ , 9])\n\n# 7. Are any of the columns giving you unexpected values?\n#    - missing values? (NA)\n\n# nope :-)\n\n# 8. Select a few key columns, make a vector of the column names\nsome.columns <- c(\"System\", \"Merged\", \"WithinMinPairs\")\ncolnames(scott)[c(2, 6, 9)] # another way to pull out a few column names\n\n# 9. Create a new data.frame with just that subset of columns\n#    from #8\n#    - do this in at least TWO different ways\nscott.subset <- scott[, some.columns]\nscott.subset2 <- scott[, c(\"System\", \"Merged\", \"WithinMinPairs\")]\nlibrary(dplyr) # for the \"select\" function\nscott.subset3 <- select(scott, System, Merged, WithinMinPairs)\nidentical(scott.subset, scott.subset2) # identical results!\nidentical(scott.subset, scott.subset3) # identical results!\n\n# 10. Create a new data.frame that is just the first 10 rows\n#     and the last 10 rows of the data from #8\nN <- nrow(scott.subset)\nscott.subset4 <- scott.subset[c(1:10, 625:634), ] # \"brute force\"\nscott.subset5 <- scott.subset[c(1:10, (N-9):N), ] # more elegant\nidentical(scott.subset4, scott.subset5)\n\n# 11. Create a new data.frame that is a random sample of half of the rows.\n# HINT: try ?sample\n\nscott.sample <- scott.subset[sample(N, N/2), ]\nnrow(scott.sample)\nrownames(scott.sample)\n\n# 12. Find a comparison in your data that is interesting to make\n#     (comparing two sets of numbers)\n#     - run a t.test for that comparison\n#     - decide whether you need a non-default test\n#       (e.g., Student's, paired)\n#     - run the t.test with BOTH the formula and \"vector\"\n#       formats, if possible\n#     - if one is NOT possible, say why you can't do it\nvar(scott$WithinMinPairs[scott$Merged == \"Merged\"])\nvar(scott$WithinMinPairs[scott$Merged == \"Unmerged\"]) # very different!\nt.test(WithinMinPairs ~ Merged, data = scott, var.equal = FALSE)\nt.test(scott$WithinMinPairs[scott$Merged == \"Merged\"], \n       scott$WithinMinPairs[scott$Merged == \"Unmerged\"], \n       var.equal = FALSE)\n\n# 13. Repeat #12 for TWO more comparisons\n#     - ALTERNATIVELY, if correlations are more interesting,\n#       do those instead of t-tests (and try both Spearman and\n#       Pearson correlations)\n\n\n# 14. Save all results from #12 and #13 in an .RData file\n\n\n# 15. Email me your version of this script, PLUS the .RData\n#     file from #14\n",
    "created" : 1483934545876.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3370476118",
    "id" : "EC7BACE3",
    "lastKnownWriteTime" : 1483932852,
    "last_content_update" : 1483932852,
    "path" : "~/projects/r_programming_bsos_winter2016/day02/day02_homework.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}